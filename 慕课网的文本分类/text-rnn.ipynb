{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From c:\\users\\administrator\\anaconda3\\envs\\muke\\lib\\site-packages\\tensorflow\\python\\compat\\v2_compat.py:96: disable_resource_variables (from tensorflow.python.ops.variable_scope) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "non-resource variables are not supported in the long term\n"
     ]
    }
   ],
   "source": [
    "# 构建计算图——LSTM模型\n",
    "#    embedding\n",
    "#    LSTM\n",
    "#    fc\n",
    "#    train_op\n",
    "# 训练流程代码\n",
    "# 数据集封装\n",
    "#    api: next_batch(batch_size)\n",
    "# 词表封装:\n",
    "#    api: sentence2id(text_sentence): 句子转换id\n",
    "# 类别的封装：\n",
    "#    api: category2id(text_category).\n",
    "\n",
    "#import tensorflow as tf\n",
    "\n",
    "import tensorflow.compat.v1 as tf\n",
    "tf.disable_v2_behavior()\n",
    "import os\n",
    "import sys\n",
    "import numpy as np\n",
    "import math\n",
    "\n",
    "#tf.logging.set_verbosity(tf.logging.INFO)  2.0格式以上变化了\n",
    "tf.compat.v1.logging.set_verbosity(tf.compat.v1.logging.ERROR)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 词向量大小\n",
    "num_embedding_size = 16,\n",
    "# 步长\n",
    "num_timesteps = 50,\n",
    "# lstm单元输出维度（又叫输出神经元数）\n",
    "num_lstm_nodes = [32, 32],\n",
    "# 层数\n",
    "num_lstm_layers = 2,\n",
    "# 全连接输出维度，最后一维\n",
    "num_fc_nodes = 32,\n",
    "batch_size = 100,\n",
    "# 控制梯度，梯度上线\n",
    "clip_lstm_grads = 1.0,\n",
    "# 学习率\n",
    "learning_rate = 0.001,\n",
    "# 词频最低下限\n",
    "num_word_threshold = 10,\n",
    "    \n",
    "\n",
    "\n",
    "\n",
    "train_file = 'D:/BaiduNetdiskDownload/Rnn图像语义全部资料/深度学习之神经网络（CNN RNN GAN）算法原理+实战/课程数据/text_classification_data/cnews.train.seg.txt'\n",
    "val_file = 'D:/BaiduNetdiskDownload/Rnn图像语义全部资料/深度学习之神经网络（CNN RNN GAN）算法原理+实战/课程数据/text_classification_data/cnews.val.seg.txt'\n",
    "test_file = 'D:/BaiduNetdiskDownload/Rnn图像语义全部资料/深度学习之神经网络（CNN RNN GAN）算法原理+实战/课程数据/text_classification_data/cnews.test.seg.txt'\n",
    "vocab_file = 'D:/BaiduNetdiskDownload/Rnn图像语义全部资料/深度学习之神经网络（CNN RNN GAN）算法原理+实战/课程数据/text_classification_data/cnews.vocab.txt'\n",
    "category_file = 'D:/BaiduNetdiskDownload/Rnn图像语义全部资料/深度学习之神经网络（CNN RNN GAN）算法原理+实战/课程数据/text_classification_data/cnews.category.txt'\n",
    "output_folder = 'D:/BaiduNetdiskDownload/Rnn图像语义全部资料/深度学习之神经网络（CNN RNN GAN）算法原理+实战/课程数据/text_classification_data/run_text_rnn'\n",
    "\n",
    "content_num_file='D:/BaiduNetdiskDownload/Rnn图像语义全部资料/深度学习之神经网络（CNN RNN GAN）算法原理+实战/课程数据/text_classification_data/content_num.txt'\n",
    "\n",
    "\n",
    "if not os.path.exists(output_folder):\n",
    "    os.mkdir(output_folder)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 下面的意思是词表和类别的封装"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "vocab_size: 77331\n",
      "[2, 4, 6, 7]\n",
      "num_classes: 10\n",
      "label:时尚, id: 5\n"
     ]
    }
   ],
   "source": [
    "class Vocab:\n",
    "    def __init__(self, filename, num_word_threshold):\n",
    "        #词典\n",
    "        self._word_to_id = {}\n",
    "        # <UNK> 的id（初始值）\n",
    "        self._unk = -1\n",
    "         # 频率下限\n",
    "        self._num_word_threshold = num_word_threshold\n",
    "        # 将词典读出来存到dict里\n",
    "        self._read_dict(filename)\n",
    "    \n",
    "    def _read_dict(self, filename):\n",
    "        with open(filename, 'r',encoding='utf-8') as f:\n",
    "            lines = f.readlines()\n",
    "        for line in lines:\n",
    "            word, frequency = line.strip('\\r\\n').split('\\t')\n",
    "            word = word\n",
    "            frequency = int(frequency)\n",
    "#             print(type(frequency)) #int\n",
    "#             print(type(self._num_word_threshold))#tuple  python的成员变量是元组，int不能和元组直接比较\n",
    "#             print(self._num_word_threshold[0])#所以要直接去取出来，不然总是报错\n",
    "            # 低于下限不要\n",
    "            if frequency < self._num_word_threshold[0]:\n",
    "                continue\n",
    "            idx = len(self._word_to_id)\n",
    "            if word == '<UNK>':\n",
    "                 # 刷新UNK的id\n",
    "                self._unk = idx\n",
    "            self._word_to_id[word] = idx\n",
    "            #print(self._unk) 0,这玩意是为了给下面做填充用的 ，padding\n",
    "    \n",
    "    # 如果没有word返回UNK\n",
    "    def word_to_id(self, word):\n",
    "        return self._word_to_id.get(word, self._unk)\n",
    "    \n",
    "    @property\n",
    "    def unk(self):\n",
    "        return self._unk\n",
    "    \n",
    "    def size(self):\n",
    "        # 返回大小\n",
    "        return len(self._word_to_id)\n",
    "    \n",
    "    def sentence_to_id(self, sentence):\n",
    "        # 分词进字典里吧id取出来变成list\n",
    "        word_ids = [self.word_to_id(cur_word) \\\n",
    "                    for cur_word in sentence.split()]\n",
    "        return word_ids\n",
    "\n",
    "\n",
    "class CategoryDict:\n",
    "    def __init__(self, filename):\n",
    "        self._category_to_id = {}\n",
    "        # 读取类别并存入字典，给每个一个id,这里一直有个错误就是读取的文件要是utf-8的，之前一直报错\n",
    "        with open(filename, 'r',encoding='utf-8') as f:\n",
    "            lines = f.readlines()\n",
    "        for line in lines:\n",
    "            category = line.strip('\\r\\n')\n",
    "            idx = len(self._category_to_id)\n",
    "            self._category_to_id[category] = idx\n",
    "    \n",
    "    def size(self):\n",
    "        return len(self._category_to_id)\n",
    "        \n",
    "    def category_to_id(self, category):\n",
    "        if not category in self._category_to_id:\n",
    "            raise Execption(\n",
    "                \"%s is not in our category list\" % category_name)\n",
    "        return self._category_to_id[category]\n",
    "        \n",
    "vocab = Vocab(vocab_file, num_word_threshold)\n",
    "vocab_size = vocab.size()\n",
    "#tf.compat.v1.logging.info('vocab_size: %d' % vocab_size)\n",
    "print('vocab_size: %d' % vocab_size)\n",
    "test_str='的 在 了 是'\n",
    "print (vocab.sentence_to_id(test_str))\n",
    "\n",
    "\n",
    "category_vocab = CategoryDict(category_file)\n",
    "num_classes = category_vocab.size()\n",
    "#tf.compat.v1.logging.info('num_classes: %d' % num_classes)\n",
    "print('num_classes: %d' % num_classes)\n",
    "test_str = '时尚'\n",
    "# tf.compat.v1.logging.info(\n",
    "#     'label: %s, id: %d' % (\n",
    "#         test_str,\n",
    "#         category_vocab.category_to_id(test_str)))\n",
    "\n",
    "print('label:%s, id: %d' % (\n",
    "        test_str,\n",
    "        category_vocab.category_to_id(test_str)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 下面的是数据集的封装"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading data from %s D:/BaiduNetdiskDownload/Rnn图像语义全部资料/深度学习之神经网络（CNN RNN GAN）算法原理+实战/课程数据/text_classification_data/cnews.train.seg.txt\n",
      "Loading data from %s D:/BaiduNetdiskDownload/Rnn图像语义全部资料/深度学习之神经网络（CNN RNN GAN）算法原理+实战/课程数据/text_classification_data/cnews.val.seg.txt\n",
      "Loading data from %s D:/BaiduNetdiskDownload/Rnn图像语义全部资料/深度学习之神经网络（CNN RNN GAN）算法原理+实战/课程数据/text_classification_data/cnews.test.seg.txt\n",
      "(array([[ 1173,   501,  3181,     0,  6906, 16789,    11,   145, 28500,\n",
      "         9681,   156,   599,   456,    82,    78,    68,    23,  1166,\n",
      "           44,   269,     1,   160,  5734,   649,  1480,   270,     1,\n",
      "        33181,     2,   501,   941, 28455,     1,    58,    63,  3181,\n",
      "          204, 63944, 52552,  6906, 15017,     1,    35,     7,   145,\n",
      "        13608,  1978,  2188,    29,     2],\n",
      "       [ 1891,     2, 49112,  1274,  8144,     6, 12688,    10,  7245,\n",
      "           17,   250,    16,  1891,     2, 49112,  1274,     1,  8144,\n",
      "            6, 12688,    10,  7245,     3, 30316,   111,   919,   916,\n",
      "         2932,     2,    41,   274,  3975,   175,   321,    50,  1385,\n",
      "          122,    19,    22,    19,   251,   164,   767,     0,     0,\n",
      "            8,     0,     9,     2,   151]]), array([8, 2]))\n",
      "(array([[ 5326,   282,  6850,   133,  6734,  6162,    83,   143, 12452,\n",
      "         3449,  1550,   153,    23,   246,   814,     4,   258,  5326,\n",
      "           24,   816,   162,   490,  3892,   905,   332,   150,   611,\n",
      "            2,  1353,    29,     1,  5326,  3892,   233,    31,   490,\n",
      "         3892,   905,  5686,  7905,     0,     4,  1410,    76,  3109,\n",
      "           65,    66,     1,    93,  6850],\n",
      "       [    0,   576,    11,  1972,  4615,   359,   150,   554,   690,\n",
      "          841,   137,   270,    76,     0,     8,  1921,     9,   265,\n",
      "           46,     2,  2025,     2,  2182,     1,    36,     8,    95,\n",
      "           43,     9,   265,    59,     2,  2182,     3,   103,    23,\n",
      "           68,    44,     1,  8672,    43,  1515, 30395,    56,   173,\n",
      "           76,   429,     1,   161,    70]]), array([6, 9]))\n",
      "(array([[  726,   872, 18931, 44934,  6378, 72620, 14333, 51553,  6378,\n",
      "        72620,  2343,   577, 26025,  1709,   203,     1, 14094,  1170,\n",
      "          427, 29615,     0,     1, 18220,   289,   323,  1812,   389,\n",
      "         5959,     3,  4539, 23196,  4269,    28, 11522,  2004,  6812,\n",
      "         2004,     0,     1,  4892,    28, 27824,  4185,     1,     4,\n",
      "        20648,    21,   790,  6075,  1770]]), array([8]))\n"
     ]
    }
   ],
   "source": [
    "class TextDataSet:\n",
    "    def __init__(self, filename, vocab, category_vocab, num_timesteps):\n",
    "        self._vocab = vocab\n",
    "        self._category_vocab = category_vocab\n",
    "        self._num_timesteps = num_timesteps\n",
    "        # matrix矩阵\n",
    "        self._inputs = []\n",
    "        # vector 列表\n",
    "        self._outputs = []\n",
    "        self._indicator = 0\n",
    "        #解析文件，就是把inputs和outputs填充的过程\n",
    "        self._parse_file(filename)\n",
    "    \n",
    "    def _parse_file(self, filename):\n",
    "        #提示我们要加载了\n",
    "        print('Loading data from %s', filename)\n",
    "        chen=[]\n",
    "        with open(filename, 'r',encoding='utf-8') as f:\n",
    "            lines = f.readlines()\n",
    "        for line in lines:\n",
    "            label, content = line.strip('\\r\\n').split('\\t')\n",
    "            #把lable转成id\n",
    "            id_label = self._category_vocab.category_to_id(label)\n",
    "            #print(content)为什么这种语句都是把所有的东西全部输出了\n",
    "            #把内容转成id\n",
    "            id_words = self._vocab.sentence_to_id(content)\n",
    "            #下面的地址是：D:\\BaiduNetdiskDownload\\Rnn图像语义全部资料\\深度学习之神经网络（CNN RNN GAN）算法原理+实战\n",
    "            #\\资料\\coding-259-master\\coding-259\\py2\\07-rnn-text-classification  我也不知道为什么会放到了代码那里去\n",
    "            with open('content_num_file','w')as f:\n",
    "                 for i in range (len(id_words)):\n",
    "                        f.write(str(id_words[i]))\n",
    "                        #f.write(\"成功了\")\n",
    "                        f.write('\\r')       \n",
    "            #chen.append(id_words)\n",
    "            #print(type(id_words)) list\n",
    "            #print(type(self._num_timesteps)) 依然是老错误，这东西是元组 tuples 所以从下一句开始出错了\n",
    "            id_words = id_words[0: self._num_timesteps[0]]#拿到前50个值，下标从0开始 \n",
    "            #print(id_words)\n",
    "            #padding 填充,这玩意百分之九十多都是0，给进去的句子是以换行为一句的，要是他没有50个词就补呗\n",
    "            padding_num = self._num_timesteps[0] - len(id_words)\n",
    "            #print(padding_num)\n",
    "            #差多少就补多少0，unk是_vocab的一个成员函数，所以可以直接调用\n",
    "            id_words = id_words + [\n",
    "                self._vocab.unk for i in range(padding_num)]\n",
    "            self._inputs.append(id_words)\n",
    "            self._outputs.append(id_label)\n",
    "        #print(self._inputs[1]) \n",
    "        #print(self._outputs)不能这么搞，会全部弄出来的。现在的input和output都是全部的资料，\n",
    "        \n",
    "        #将输入数据（列表的列表，元组的元组，元组的列表等）转换为矩阵形式\n",
    "        self._inputs = np.asarray(self._inputs, dtype = np.int32)\n",
    "        self._outputs = np.asarray(self._outputs, dtype = np.int32)\n",
    "        #随机化\n",
    "        self._random_shuffle()\n",
    "    \n",
    "    def _random_shuffle(self):\n",
    "        p = np.random.permutation(len(self._inputs))\n",
    "       # print(p) 生成了len(self._inputs)那么长的随机数据,然后赋值给intput和outputs,\n",
    "       #说不清，看下一个cell，我做了一个实验，output是lable值，应该只是把随机种子赋值过来，值还是自己的值\n",
    "        self._inputs = self._inputs[p]\n",
    "        self._outputs = self._outputs[p]\n",
    "    \n",
    "    #获得下一个minbatch\n",
    "    def next_batch(self, batch_size):\n",
    "        # 当获取的指针超过数据集大小时，归0\n",
    "        end_indicator = self._indicator + batch_size\n",
    "        if end_indicator > len(self._inputs):\n",
    "            self._random_shuffle()\n",
    "            self._indicator = 0\n",
    "            end_indicator = batch_size\n",
    "        # 归0还超过就报错\n",
    "        if end_indicator > len(self._inputs):\n",
    "            raise Execption(\"batch_size: %d is too large\" % batch_size)\n",
    "        \n",
    "        #取值\n",
    "        batch_inputs = self._inputs[self._indicator: end_indicator]\n",
    "        batch_outputs = self._outputs[self._indicator: end_indicator]\n",
    "        self._indicator = end_indicator\n",
    "        return batch_inputs, batch_outputs\n",
    "\n",
    "    \n",
    "    #分别把train val test的数据传进去\n",
    "train_dataset = TextDataSet(\n",
    "    train_file, vocab, category_vocab, num_timesteps) \n",
    "val_dataset = TextDataSet(\n",
    "    val_file, vocab, category_vocab, num_timesteps)\n",
    "test_dataset = TextDataSet(\n",
    "    test_file, vocab, category_vocab, num_timesteps)\n",
    "\n",
    "print (train_dataset.next_batch(2))\n",
    "print (val_dataset.next_batch(2))\n",
    "print (test_dataset.next_batch(1))  \n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2 4 0 1 3]\n",
      "[ 6  7  8  9 10]\n",
      "<class 'numpy.ndarray'>\n",
      "[ 8 10  6  7  9]\n"
     ]
    }
   ],
   "source": [
    "chen= np.random.permutation(5)\n",
    "print(chen)\n",
    "\n",
    "chen_inputs=[6,7,8,9,10]\n",
    "#print(type(chen_inputs))#list\n",
    "\n",
    "chen_inputs=np.asarray(chen_inputs, dtype = np.int32)\n",
    "print(chen_inputs) #[6,7,8,9,10]\n",
    "#print(type(chen_inputs)) #矩阵\n",
    "\n",
    "chen_inputs=chen_inputs[chen]\n",
    "print(chen_inputs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "8\n",
      "8\n"
     ]
    }
   ],
   "source": [
    "hao=8\n",
    "print(hao)\n",
    "print(str(hao))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### enbedding与one-hot编码 很不错的讲解\n",
    "\n",
    "(46条消息) embedding_lookup的学习笔记_FBeetle的博客-CSDN博客\n",
    "https://blog.csdn.net/hit0803107/article/details/98377030"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "embedding:\n",
      "[[1 0 0 0 0 0 0 0]\n",
      " [0 1 0 0 0 0 0 0]\n",
      " [0 0 1 0 0 0 0 0]\n",
      " [0 0 0 1 0 0 0 0]\n",
      " [0 0 0 0 1 0 0 0]\n",
      " [0 0 0 0 0 1 0 0]\n",
      " [0 0 0 0 0 0 1 0]\n",
      " [0 0 0 0 0 0 0 1]]\n",
      "\n",
      " var1:\n",
      "[1, 2, 6, 4, 2, 5, 7]\n",
      "\n",
      "projecting result:\n",
      "[[0 1 0 0 0 0 0 0]\n",
      " [0 0 1 0 0 0 0 0]\n",
      " [0 0 0 0 0 0 1 0]\n",
      " [0 0 0 0 1 0 0 0]\n",
      " [0 0 1 0 0 0 0 0]\n",
      " [0 0 0 0 0 1 0 0]\n",
      " [0 0 0 0 0 0 0 1]]\n",
      "\n",
      " _var2:\n",
      "[[1, 4], [6, 3], [2, 5]]\n",
      "\n",
      " _projecting result:\n",
      "[[[0 1 0 0 0 0 0 0]\n",
      "  [0 0 0 0 1 0 0 0]]\n",
      "\n",
      " [[0 0 0 0 0 0 1 0]\n",
      "  [0 0 0 1 0 0 0 0]]\n",
      "\n",
      " [[0 0 1 0 0 0 0 0]\n",
      "  [0 0 0 0 0 1 0 0]]]\n"
     ]
    }
   ],
   "source": [
    "\n",
    "#不要删除，这个是下面的测试\n",
    "input_ids = tf.placeholder(dtype=tf.int32, shape=[None])\n",
    "_input_ids = tf.placeholder(dtype=tf.int32, shape=[3, 2])\n",
    "\n",
    "\n",
    "embedding_param = tf.Variable(np.identity(8, dtype=np.int32))   # 生成一个8x8的单位矩阵\n",
    "input_embedding = tf.nn.embedding_lookup(embedding_param, input_ids)\n",
    "_input_embedding = tf.nn.embedding_lookup(embedding_param, _input_ids)\n",
    "\n",
    "sess = tf.InteractiveSession()\n",
    "## 含有tf.Variable的环境下，因为tf中建立的变量是没有初始化的，也就是在debug时还不是一个tensor量，\n",
    "    #而是一个Variable变量类型\n",
    "sess.run(tf.global_variables_initializer())\n",
    "\n",
    "\n",
    "print('embedding:')\n",
    "print(embedding_param.eval())\n",
    "\n",
    "var1 = [1, 2, 6, 4, 2, 5, 7]\n",
    "print('\\n var1:')\n",
    "print(var1)\n",
    "\n",
    "print('\\nprojecting result:')\n",
    "print(sess.run(input_embedding, feed_dict={input_ids: var1}))\n",
    "\n",
    "var2 = [[1, 4], [6, 3], [2, 5]]\n",
    "print('\\n _var2:')\n",
    "print(var2)\n",
    "\n",
    "print('\\n _projecting result:')\n",
    "print(sess.run(_input_embedding, feed_dict={_input_ids: var2}))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.6123724356957945\n"
     ]
    }
   ],
   "source": [
    "chen1= 1.0 / math.sqrt(8 / 3.0)\n",
    "print(chen1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<tf.Variable 'Variable:0' shape=() dtype=float32_ref>\n",
      "<tf.Variable 'Variable_1:0' shape=() dtype=float32_ref>\n"
     ]
    }
   ],
   "source": [
    "#通过这个可以看出来tf.variable的东西是三个变量的，\n",
    "x=tf.Variable(0.)\n",
    "y=tf.Variable(1.)\n",
    "print(x)\n",
    "print(y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "下面到处是这种 with tf.variable_scope "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "Error converting shape to a TensorShape: Dimension value must be integer or None or have an __index__ method, got value '(100,)' with type '<class 'tuple'>'.",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32mc:\\users\\administrator\\anaconda3\\envs\\muke\\lib\\site-packages\\tensorflow\\python\\eager\\execute.py\u001b[0m in \u001b[0;36mmake_shape\u001b[1;34m(v, arg_name)\u001b[0m\n\u001b[0;32m    210\u001b[0m   \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 211\u001b[1;33m     \u001b[0mshape\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtensor_shape\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mas_shape\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mv\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    212\u001b[0m   \u001b[1;32mexcept\u001b[0m \u001b[0mTypeError\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\users\\administrator\\anaconda3\\envs\\muke\\lib\\site-packages\\tensorflow\\python\\framework\\tensor_shape.py\u001b[0m in \u001b[0;36mas_shape\u001b[1;34m(shape)\u001b[0m\n\u001b[0;32m   1234\u001b[0m   \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1235\u001b[1;33m     \u001b[1;32mreturn\u001b[0m \u001b[0mTensorShape\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mshape\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1236\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\users\\administrator\\anaconda3\\envs\\muke\\lib\\site-packages\\tensorflow\\python\\framework\\tensor_shape.py\u001b[0m in \u001b[0;36m__init__\u001b[1;34m(self, dims)\u001b[0m\n\u001b[0;32m    757\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdims\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m(\u001b[0m\u001b[0mtuple\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mlist\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m  \u001b[1;31m# Most common case.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 758\u001b[1;33m       \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_dims\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[0mDimension\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0md\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0md\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mdims\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    759\u001b[0m     \u001b[1;32melif\u001b[0m \u001b[0mdims\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\users\\administrator\\anaconda3\\envs\\muke\\lib\\site-packages\\tensorflow\\python\\framework\\tensor_shape.py\u001b[0m in \u001b[0;36m<listcomp>\u001b[1;34m(.0)\u001b[0m\n\u001b[0;32m    757\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdims\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m(\u001b[0m\u001b[0mtuple\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mlist\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m  \u001b[1;31m# Most common case.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 758\u001b[1;33m       \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_dims\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[0mDimension\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0md\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0md\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mdims\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    759\u001b[0m     \u001b[1;32melif\u001b[0m \u001b[0mdims\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\users\\administrator\\anaconda3\\envs\\muke\\lib\\site-packages\\tensorflow\\python\\framework\\tensor_shape.py\u001b[0m in \u001b[0;36m__init__\u001b[1;34m(self, value)\u001b[0m\n\u001b[0;32m    205\u001b[0m                       \u001b[1;34m\"an __index__ method, got value '{0!r}' with type '{1!r}'\"\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 206\u001b[1;33m                       .format(value, type(value))), None)\n\u001b[0m\u001b[0;32m    207\u001b[0m       \u001b[1;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_value\u001b[0m \u001b[1;33m<\u001b[0m \u001b[1;36m0\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\users\\administrator\\anaconda3\\envs\\muke\\lib\\site-packages\\six.py\u001b[0m in \u001b[0;36mraise_from\u001b[1;34m(value, from_value)\u001b[0m\n",
      "\u001b[1;31mTypeError\u001b[0m: Dimension value must be integer or None or have an __index__ method, got value '(100,)' with type '<class 'tuple'>'",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[1;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32mC:\\Users\\ADMINI~1\\AppData\\Local\\Temp/ipykernel_20216/3074010229.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m    132\u001b[0m             (train_op, global_step))\n\u001b[0;32m    133\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 134\u001b[1;33m \u001b[0mplaceholders\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mmetrics\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mothers\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mcreate_model\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mvocab_size\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mnum_classes\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    135\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    136\u001b[0m \u001b[0minputs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0moutputs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mkeep_prob\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mplaceholders\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\Users\\ADMINI~1\\AppData\\Local\\Temp/ipykernel_20216/3074010229.py\u001b[0m in \u001b[0;36mcreate_model\u001b[1;34m(vocab_size, num_classes)\u001b[0m\n\u001b[0;32m      5\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      6\u001b[0m     \u001b[1;31m#占位符 （三个参数分别是类型，形状（batch_size行，num_timesteps列，没有就是不固定），名字）\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 7\u001b[1;33m     \u001b[0minputs\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtf\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mplaceholder\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtf\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mint32\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m(\u001b[0m\u001b[0mbatch_size\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mnum_timesteps\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      8\u001b[0m     \u001b[0moutputs\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtf\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mplaceholder\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtf\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mint32\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m(\u001b[0m\u001b[0mbatch_size\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      9\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\users\\administrator\\anaconda3\\envs\\muke\\lib\\site-packages\\tensorflow\\python\\ops\\array_ops.py\u001b[0m in \u001b[0;36mplaceholder\u001b[1;34m(dtype, shape, name)\u001b[0m\n\u001b[0;32m   3177\u001b[0m                        \"eager execution.\")\n\u001b[0;32m   3178\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 3179\u001b[1;33m   \u001b[1;32mreturn\u001b[0m \u001b[0mgen_array_ops\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mplaceholder\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdtype\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mdtype\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mshape\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mshape\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mname\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mname\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   3180\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   3181\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\users\\administrator\\anaconda3\\envs\\muke\\lib\\site-packages\\tensorflow\\python\\ops\\gen_array_ops.py\u001b[0m in \u001b[0;36mplaceholder\u001b[1;34m(dtype, shape, name)\u001b[0m\n\u001b[0;32m   6721\u001b[0m   \u001b[1;32mif\u001b[0m \u001b[0mshape\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   6722\u001b[0m     \u001b[0mshape\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 6723\u001b[1;33m   \u001b[0mshape\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0m_execute\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmake_shape\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mshape\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m\"shape\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   6724\u001b[0m   _, _, _op, _outputs = _op_def_library._apply_op_helper(\n\u001b[0;32m   6725\u001b[0m         \"Placeholder\", dtype=dtype, shape=shape, name=name)\n",
      "\u001b[1;32mc:\\users\\administrator\\anaconda3\\envs\\muke\\lib\\site-packages\\tensorflow\\python\\eager\\execute.py\u001b[0m in \u001b[0;36mmake_shape\u001b[1;34m(v, arg_name)\u001b[0m\n\u001b[0;32m    211\u001b[0m     \u001b[0mshape\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtensor_shape\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mas_shape\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mv\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    212\u001b[0m   \u001b[1;32mexcept\u001b[0m \u001b[0mTypeError\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 213\u001b[1;33m     \u001b[1;32mraise\u001b[0m \u001b[0mTypeError\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"Error converting %s to a TensorShape: %s.\"\u001b[0m \u001b[1;33m%\u001b[0m \u001b[1;33m(\u001b[0m\u001b[0marg_name\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0me\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    214\u001b[0m   \u001b[1;32mexcept\u001b[0m \u001b[0mValueError\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    215\u001b[0m     raise ValueError(\"Error converting %s to a TensorShape: %s.\" % (arg_name,\n",
      "\u001b[1;31mTypeError\u001b[0m: Error converting shape to a TensorShape: Dimension value must be integer or None or have an __index__ method, got value '(100,)' with type '<class 'tuple'>'."
     ]
    }
   ],
   "source": [
    "def create_model( vocab_size, num_classes):\n",
    "    #步长\n",
    "    #num_timesteps = num_timesteps\n",
    "    #batch_size = batch_size\n",
    "    \n",
    "    #占位符 （三个参数分别是类型，形状（batch_size行，num_timesteps列，没有就是不固定），名字）\n",
    "    inputs = tf.placeholder(tf.int32, (batch_size, num_timesteps))\n",
    "    outputs = tf.placeholder(tf.int32, (batch_size, ))\n",
    "    \n",
    "    \n",
    "#     Dropout就是在不同的训练过程中随机扔掉部分神经元。也就是让某个神经元的激活值以一定的概率p，\n",
    "#     让其停止工作，这次训练过程中不更新权值，也不参加神经网络的计算。\n",
    "#     tf.nn.dropout(x, keep_prob, noise_shape=None, seed=None,name=None)\n",
    "#     第一个参数x：指输入第二个参数keep_prob: 设置神经元被选中的概率,在初始化时keep_prob是一个占位符,\n",
    "#     keep_prob = tf.placeholder(tf.float32) 。\n",
    "#     tensorflow在run时设置keep_prob具体的值，例如keep_prob: 0.5\n",
    "\n",
    "    \n",
    "   \n",
    "    keep_prob = tf.placeholder(tf.float32, name = 'keep_prob')\n",
    "    \n",
    "    # 保存模型运行进度\n",
    "    global_step = tf.Variable(\n",
    "        tf.zeros([], tf.int64), name = 'global_step', trainable=False)\n",
    "    \n",
    "     # embedding的值初始化到-1到1\n",
    "    embedding_initializer = tf.random_uniform_initializer(-1.0, 1.0)\n",
    "    \n",
    "    #写成下面这样会好理解一些 scope理解成作用域，就是范围，与get_variable配合使用\n",
    "    with tf.variable_scope('embedding', initializer = embedding_initializer):\n",
    "        \n",
    "        #把上面的[-1，1]的embeddings弄成一个vocab_size长，num_embedding_size宽的，重新赋给自己\n",
    "        embeddings = tf.get_variable( 'embedding',[vocab_size, num_embedding_size],tf.float32)\n",
    "        \n",
    "        # [1, 10, 7] -> [embeddings[1], embeddings[10], embeddings[7]] \n",
    "        #将一个个词转化成embedding，看上面的enbedding与one-hot编码，你也别管怎么转了\n",
    "        embed_inputs = tf.nn.embedding_lookup(embeddings, inputs)\n",
    "        \n",
    "     # 初始化随机值,先别管是什么，反正就是一个数字，看上面的测试\n",
    "    scale = 1.0 / math.sqrt(num_embedding_size + num_lstm_nodes[-1]) / 3.0\n",
    "    print(\"num_embedding_size:\"+num_embedding_size+\"/r\")\n",
    "    print(\"num_lstm_nodes[-1]:\"+num_lstm_nodes[-1]+\"/r\")\n",
    "    print(scale)\n",
    "    \n",
    "    \n",
    "    lstm_init = tf.random_uniform_initializer(-scale, scale)\n",
    "    with tf.variable_scope('lstm_nn', initializer = lstm_init):\n",
    "        # 双层所以写一个list\n",
    "        cells = []\n",
    "        for i in range(num_lstm_layers):\n",
    "            #basic+lstm+cell tf中的api直接实现了，num_lstm_nodes[i]是输入的大小\n",
    "            #用于存储LSTM单元的state_size,zero_state和output state的元组。按顺序存储两个元素(c,h),\n",
    "            #其中c是隐藏状态，h是输出。只有在state_is_tuple=True是才使用。\n",
    "            cell = tf.contrib.rnn.BasicLSTMCell(num_lstm_nodes[i],state_is_tuple = True)\n",
    "            #把上一层的cell作为这一层的输入，进行dropout\n",
    "            cell = tf.contrib.rnn.DropoutWrapper(cell,output_keep_prob = keep_prob)\n",
    "            cells.append(cell)\n",
    "        #把两个cell拼接到一起\n",
    "        cell = tf.contrib.rnn.MultiRNNCell(cells)\n",
    "        \n",
    "        # 初始化隐含状态为0\n",
    "        initial_state = cell.zero_state(batch_size, tf.float32)\n",
    "        \n",
    "        #是tensorflow封装的用来实现递归神经网络（RNN）的函数\n",
    "        # rnn_outputs: [batch_size, num_timesteps, lstm_outputs[-1]]\n",
    "        rnn_outputs, _ = tf.nn.dynamic_rnn(\n",
    "            cell, embed_inputs, initial_state = initial_state)\n",
    "        #取最后一层，细化的东西再说吧\n",
    "        # [batch_size, 1, lstm_outputs[-1]（32）]\n",
    "        last = rnn_outputs[:, -1, :]\n",
    "    \n",
    "    #类似于均匀分布的东西\n",
    "    fc_init = tf.uniform_unit_scaling_initializer(factor=1.0)\n",
    "    \n",
    "    # 输出和全连接层拼接\n",
    "    with tf.variable_scope('fc', initializer = fc_init):\n",
    "        # 上面lstm的cell的输出拿过来\n",
    "        # 激活函数用relu\n",
    "        # 第二个参数是输出维度大小，就是最后一个参数,这里是没有变化的\n",
    "        fc1 = tf.layers.dense(last, \n",
    "                              num_fc_nodes,\n",
    "                              activation = tf.nn.relu,\n",
    "                              name = 'fc1')\n",
    "        # dropout\n",
    "        fc1_dropout = tf.contrib.layers.dropout(fc1, keep_prob)\n",
    "        \n",
    "        # 在经过一层全连接映射到多少num_classes个类别上 \n",
    "        logits = tf.layers.dense(fc1_dropout,\n",
    "                                 num_classes,\n",
    "                                 name = 'fc2')\n",
    "    \n",
    "    \n",
    "    with tf.name_scope('metrics'):\n",
    "        #交叉熵损失函数，类别与标签的交叉熵\n",
    "        softmax_loss = tf.nn.sparse_softmax_cross_entropy_with_logits(logits = logits, labels = outputs)\n",
    "        # 计算均值\n",
    "        loss = tf.reduce_mean(softmax_loss)\n",
    "        # [0, 1, 5, 4, 2] -> argmax: 2\n",
    "        # 这里第一个参数就是softmax，第二个参数意思是取第几维最大，比如是1的话就是在第一维里去最大\n",
    "        y_pred = tf.argmax(tf.nn.softmax(logits),\n",
    "                           1, \n",
    "                           output_type = tf.int32)\n",
    "        \n",
    "        # 计算准确率，看看算对多少个\n",
    "        correct_pred = tf.equal(outputs, y_pred)\n",
    "        \n",
    "        # 求均值，tf.cast  将数据转换成 tf.float32 类型\n",
    "        accuracy = tf.reduce_mean(tf.cast(correct_pred, tf.float32))\n",
    "    \n",
    "    #定义op(operation) 而且是train的op\n",
    "    with tf.name_scope('train_op'):\n",
    "        # 获得所有可训练的变量（优化器优化列表中的变量）\n",
    "        tvars = tf.trainable_variables()\n",
    "        \n",
    "        # 看看这些变量叫啥\n",
    "        for var in tvars:\n",
    "            tf.logging.info('variable name: %s' % (var.name))\n",
    "            \n",
    "        # tf.gradients求导（梯度下降，对每个训练变量求偏导，每个方向都往下走）\n",
    "        # 不超过hps.clip_lstm_grads，防止梯度爆炸\n",
    "        grads, _ = tf.clip_by_global_norm(tf.gradients(loss, tvars), clip_lstm_grads)\n",
    "        \n",
    "        # Adam优化器\n",
    "        optimizer = tf.train.AdamOptimizer(learning_rate)\n",
    "        \n",
    "        # 这里是应用，将梯度grads作用到变量上（看这个名字）,让函数吧global_step回调\n",
    "        train_op = optimizer.apply_gradients(\n",
    "            zip(grads, tvars), global_step = global_step)#zip将变量和梯度绑定在一起，先记住着吧\n",
    "    \n",
    "    return ((inputs, outputs, keep_prob),\n",
    "            (loss, accuracy),\n",
    "            (train_op, global_step))\n",
    "\n",
    "placeholders, metrics, others = create_model(vocab_size, num_classes)\n",
    "\n",
    "inputs, outputs, keep_prob = placeholders\n",
    "loss, accuracy = metrics\n",
    "train_op, global_step = others\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "unsupported operand type(s) for +: 'int' and 'tuple'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32mC:\\Users\\ADMINI~1\\AppData\\Local\\Temp/ipykernel_20216/1558230444.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     15\u001b[0m     \u001b[1;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mnum_train_steps\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     16\u001b[0m         batch_inputs, batch_labels = train_dataset.next_batch(\n\u001b[1;32m---> 17\u001b[1;33m             batch_size)\n\u001b[0m\u001b[0;32m     18\u001b[0m         \u001b[1;31m# 那三个占位符输进去\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     19\u001b[0m         \u001b[1;31m# 计算loss, accuracy, train_op, global_step的图\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\Users\\ADMINI~1\\AppData\\Local\\Temp/ipykernel_20216/3421016663.py\u001b[0m in \u001b[0;36mnext_batch\u001b[1;34m(self, batch_size)\u001b[0m\n\u001b[0;32m     64\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mnext_batch\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mbatch_size\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     65\u001b[0m         \u001b[1;31m# 当获取的指针超过数据集大小时，归0\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 66\u001b[1;33m         \u001b[0mend_indicator\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_indicator\u001b[0m \u001b[1;33m+\u001b[0m \u001b[0mbatch_size\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     67\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mend_indicator\u001b[0m \u001b[1;33m>\u001b[0m \u001b[0mlen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_inputs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     68\u001b[0m             \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_random_shuffle\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mTypeError\u001b[0m: unsupported operand type(s) for +: 'int' and 'tuple'"
     ]
    }
   ],
   "source": [
    "init_op = tf.global_variables_initializer()\n",
    "#keep_prob的 drop的概率\n",
    "train_keep_prob_value = 0.8\n",
    "test_keep_prob_value = 1.0\n",
    "\n",
    "#跑多少次\n",
    "#num_train_steps = 10000\n",
    "num_train_steps = 100\n",
    "\n",
    "# Train: 99.7%\n",
    "# Valid: 92.7%\n",
    "# Test:  93.2%\n",
    "with tf.Session() as sess:\n",
    "    sess.run(init_op)\n",
    "    for i in range(num_train_steps):\n",
    "        batch_inputs, batch_labels = train_dataset.next_batch(\n",
    "            batch_size)\n",
    "        # 那三个占位符输进去\n",
    "        # 计算loss, accuracy, train_op, global_step的图\n",
    "        outputs_val = sess.run([loss, accuracy, train_op, global_step],\n",
    "                               feed_dict = {\n",
    "                                   inputs: batch_inputs,\n",
    "                                   outputs: batch_labels,\n",
    "                                   keep_prob: train_keep_prob_value,\n",
    "                               })\n",
    "        loss_val, accuracy_val, _, global_step_val = outputs_val\n",
    "         # 两百个batch输出一次\n",
    "        if global_step_val % 20 == 0:\n",
    "            print(\"Step: %5d, loss: %3.3f, accuracy: %3.3f\"\n",
    "                            % (global_step_val, loss_val, accuracy_val))\n",
    "            \n",
    "            \n",
    "        # 一千个batch校验一次\n",
    "        if global_step_val % 1000 == 0:\n",
    "            accuracy_eval = eval_holdout(sess, accuracy, val_dataset, hps.batch_size)\n",
    "            accuracy_test = eval_holdout(sess, accuracy, test_dataset, hps.batch_size)\n",
    "            print(\"Step: %5d, val_accuracy: %3.3f, test_accuracy: %3.3f\"\n",
    "                            % (global_step_val, accuracy_eval, accuracy_test))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
